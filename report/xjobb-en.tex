\documentclass[a4paper,11pt]{kth-mag}
\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage{lmodern}
\usepackage[utf8]{inputenc}
\usepackage[swedish,english]{babel}
\usepackage{modifications}
\usepackage{graphicx}
\usepackage{blindtext}
\usepackage{tabularx}

\title{A study of Sudoku solving algorithms}

\subtitle{}
\foreigntitle{En studie om Sudokulösningsalgorithmer}
\author{Patrik Berggren \and David Nilsson}
\date{April 2012}
\blurb{Bachelor's Thesis at NADA\\Supervisor: Alexander Baltatzis\\Examiner: Mårten Björkman}
\trita{TRITA xxx yyyy-nn}
\begin{document}
\frontmatter
\pagestyle{empty}
\removepagenumbers
\maketitle
\selectlanguage{english}
\begin{abstract}
This is a bachelor thesis that studies and compare four different Sudoku
solving algorithms.
Those algorithms are rule-based, backtrack, and Boltzmann machine.
 The comparison consisted of measuring the algorithms, including
variations of the algorithms, against a database of 49 151 different 17-clue Sudoku
puzzles.
The results show that ...
\end{abstract}
\clearpage
\begin{foreignabstract}{swedish}
TODO: 
\end{foreignabstract}
\clearpage
\tableofcontents*
\mainmatter
\pagestyle{newchap}

\chapter{Introduction}
%TODO, pabergg
Sudoku is a game that under recent years have gained popularity throughout the world.
Many newspaper today contain sudoku puzzles and there are also competitions devoted 
to sudoku solving.
It is therefore of interest to study how one can solve, generate and rate such puzzles by the help of computers algorithms.

\section{Problem specification}
There are multiple algorithms that can solve sudoku puzzles.
In this report we discuss three different algorithms and compare them in their abbilities.
The primary focus is on solving ability, but some results regarding difficulty rating are also presented.
The solving ability that is of primarly focus is the time distribution among the tested puzzles for each algorithm, but some discussion regarding other topics will also be done.
Those other algorithms primarly include how well the algorithms are suited for parallelising and well the algorithms can be used in generating puzzles.
The algorithms that will be studied are backtrack solver, rulebased solver and Boltzmann machines. 
This algorithms and their implementations are explained in more detail in section 2. 
>>>>>>> Some minor changes and some text added to analyses

\section{Scope}
As this project is quite limited in time available and in expected scope, 
there are several limitations on what can and will be done. 
These are the limitations of the project:
\begin{itemize}
    \item Limited number of algorithms: 
Because of the time available is limited we have choosen to limit the number of algorithms studied to three. 
    \item Optimization: 
All algorithms are implemented by ourselfes and optimization is therefore an issue. 
We have therefore only aimed for exploring the underlying ideas of the algorithms and not the algorithms themselfes.
There is however the possibility that some dramatic optimization could have been done that affect the result, and we have therefore been quite cautios about this particular issue. There is however still a lot of things that can be determined with certainty.

    \item Special sudokus: There are several variations of sudoku including different sizes of the grid.
This thesis will however be limited to the study of ordinary sudoku, which is 9 by 9 grids.
    \item The results of this thesis will be applicable to other areas apart from sudoku related topics, but those will only be mentioned briefly and no extensive study regarding the use of the results in other areas will be done. 
\end{itemize}

\section{Purpose}
As already mentioned, sudoku is today a popular game throughout the world and it appears in multiple medias, including websites, newspapers and books. 
As a result, it is of interest to find effective sudoku solving and generating algorithms. 
For most purposes there already exist satisfactory algorithms, and as a result one might struggle to see the use in studying sudoku solving algorithms. 
There is however still some value in studying sudoku solving algorithms as it might reveal how one can deal with harder variations of sudoku, such as puzzles with a 16 by 16 grid. 
Sudoku is also, as will be discussed in section 2, an NP-Complete problem which means that it is one of many computational difficult problems. 
One hope of this study is to contribute to the discussion about how one can deal with such puzzles. 
We will as mentioned in section 1.2 not discuss how our algorithms could be used in other areas, but seen as many NP-Complete problems can frequently be transformed into others it is still plausible that our result could be valueable. 
Sudoku is one of those NP-complete problems \cite{complexity} which briefly speaking means that 

\chapter{Background}
The background gives an introduction to Sudoku solving and the various approaches to creating efficient solvers.

\section{Sudoku fundamentals}
%list rules and puzzle difficulties, puzzle uniqeness
A Sudoku game consists of a 9x9 grid of numbergs, each belonging to the range 1-9.
Initially a subset of the grid is revealed and the goal is to fill the reamining grid with valid numbers.
The grid is guarded by certain rules restricting which values that are valid insertions, with the initial subset always being valid.
The three main rules are: row and columns can only contain all 1-9 digits exactly once, which also applies to each one of the nine 3x3 subgrids \cite{17clueProof}.
In order to be regarded as a proper Sudoku puzzle it is also required that a unique solution exists, a property which can be analyzed by studying the size of the initial subset and solving for all possible solutions.

The size of the given subset, typically referred to as the number of clues, determine the difficulty of finding all grid values and thereby solving the Sudoku.
Commonly the number of clues are reduced for increased difficulty with difficulty levels such as "easy", "medium" and "hard" being common ratings in newpapers.
Rating puzzles is however not just that simple but requires more extensive analysis, something which has been studied separately (http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6263\&rep=rep1\&type=pdf\#page=91).

There is however a lower limit on the number of clues given that results in a unique solution.
This limit was proven to be 17 \cite{17clueProof}, limiting the interesting number of clues to the range of 17-80.

\section{Computational perspective}
%list computational analysis and complexity, also different solving methods
Sudoku solving is an research area in computer science and mathematics, with areas such as solving, puzzle difficulty rating and puzzle generation being researched \cite{stochastic}(http://www.springerlink.com/index/L14T86X63XQ7402T.pdf, "Research on Construting of Sudoku Puzzles") \cite{generation}.

The problem of solving $n^2 * n^2$ Sudoku puzzles is NP-complete \cite{complexity}.
While being theoretically interesting a an result it has also motivated research into heurstics, resulting in a wide range of available solving methods.
Some of these algorithms include backtrack \cite{searchBased}, rule-based \cite{techniques}, cultural genetic with variations\cite{stochastic}, and Boltzmann machines \cite{boltzmann}.

Given the large variety of solvers available it is interesting to group them together with similar features in mind and try to make generic statements about their performance and other aspects.
One selection critiera is their underlying method of traversing the search space, in this case deterministic and stochastic methods.
Deterministic solvers include backtrack and rule-based.
The typical layout of these is a predetermined selection of rules and a deterministic way of traversing all possible solutions.
They can be seen as performing discrete steps and at every moment some transformation is applied in a deterministic way.
Stochastic solvers include genetic algorithms and Boltzmann machines.
They are typically based on a different stochastic selection criteria that decides how candidate solutions are constructed and how the general search path is built up.
While providing more flexibility and more a more generic approach to Sudoku solving there are weaker guarantees surrounding execution time until completion, since a solution can become apparent at any moment, but also take longer time \cite{stochastic}.

\section{Evaluated algorithms}
Given the large amount of different algorithms available it is necessary to reduce the candidates, while still providing a quantitative study with broad results.
With these requirements in mind, three different algorithms were chosen: backtrack, rule-based and Boltzmann machine.
These represent different groups of solvers and were all possible to implement within a reasonable timeframe.
A short description is given below with further in depth studies in the following subsections.
\begin{itemize}
    \item Backtrack: Backtrack is probably the most basic Sudoku solving strategy for computer algorithms.
It is a kind of a bruteforce method which tries different numbers and if it fails it backtracks and try a different number.
    \item Rule-based: This method consists of using several rules that logically proves that a square either must have a certain number or roles out numbers that are impossible (which for instance could lead to a square with only one possible number).
This method is very similar to how humans solve Sudoku and the rules used is in fact derived from human solving methods.
    \item Boltzmann machine: Modeling a Sudoku by using a constraint solving artificial neural network.
Puzzles are seen as constraints describing which nodes that can not be connected to each other.
These constraints are encoded into weights of an artificial neural network and then solved until a valid solution appears, with active nodes indicating chosen digits.
\end{itemize}

\subsection{Backtrack}
The backtracking algorithm for solving Sudoku is a bruteforce method.
One might view it as guessing which numbers goes where.
When a deadend is reached, the algorithm backtracks to a earlier guess and tries something else.
This means that the backtracking algorithm does an extensive search to find a solution.
The main drawback to this algorithm is that it is exponential in time complexity, but will on the other hand always find a solution sooner or later if one exists.
This method may also be used to determine if a solution is unique for a puzzle as the algorithm can easily be modified to continue searching after finding one solution.
As a result it could be used to generate valid sudoku puzzles (with unique solutions).\\
Parallelising is something that could improve this algorithm considerable.
The way this could be done is by searching different searchpaths in parallel. This is possible as the search three is fixed and only determined by the puzzle.\\
There are several interesting variations of this algorithm that might prove to vary in effiecency.
The main variations regards in which order the squares are visited.
One might for instance search the squares in random order or in some order that is defined by the structure of the Sudoku.
One might for instance first fill in all possible numbers in each empty square and then begin with the least number of candidates, or one might want to finish nearly completed rows, columns or boxes.
One might also vary how the guesses are done.
Some rules regarding how the guesses are done might lead to a higher probability to find a solution which in turn will make the whole algorithm faster as the solutions will be found earlier in the search.

\subsection{Rule-based}
This algorithm relies on the logic rules that humans use to solve Sudoku.
Those rules works as proofs and either determines that a square must be a certain number or prooves that a certain square can not contain a certain number.
The rules that are used are as follows:
\begin{description}
    \item[Naked Single] 
    This means that a square only have one candidate number.
    \item[Hidden Single] 
    If a row, column or box contains only one square which can hold a specific number then that number must go into that square.
    \item[Naked pair] 
    If a row, column or box contains two squares which each only have two specific candidates.
If one such pair exists, then all occurences of these two candidates may be removed from all squares that share a row, column or box with both of the squares in the pair.
This concept can also be extended to three or more squares.
    \item[Hidden pair]
    If a row, column or box contains only two squares which can hold two specific candidates, then those squares are a hidden pair.
It is hidden because those squares might also include several other candidates.
Since one already know which two numbers have to go into the two squares one might remove other candidates for those two squares.
Those squares will now be a naked pair and one could therefore apply the rule for removal of further candidates.
Similar to naked pairs this concept may also be extended to three or more squares.

    \item[X-wing/swordfish/jellyfish/squirmbag]
    These are all names of a more general rule that concerns a loop.
Below is a example of a so called swordfish (figure 1 and 2).
In figure 1 the squares which have the digit 4 as a candidate have been marked.
The so called swordfish is shown in figure 2.
The logical reasoning one might use with a swordfish is that since the configurations of the fours in the swordfish can only take two configurations and both of them will cover the same set of columns and rows.
The other squares in those rows and columns with the digit 4 as a candidate may therefore remove 4 as a candidate.
Swordfish is the special case of this concept where 3 rows and columns are involved in the loop, but the concept also applies to loops within 2 rows and columns (X-wing) and even loops with more than 3 rows and columns involved (jellyfish and squirmbag).

\includegraphics{images/swordfish1.png}\\
\includegraphics{images/swordfish2.png}

\end{description}

There are other rules that could be used but due to limitations these are the only implemented ones.
There are several different aspects that are interesting to note about this algorithm.
First of all the time complexity of the algorithm will be polynomial given that a solution can be found using these rules.
This is a simple consequence of the fact that each rule only take polynomial time to check and there are only polynomial many squares to fill in.
There are however no guarantee that a solution will be found and if no rule match it is impossible for the algorithm to continue.
One way around this is to combine this algorithm with the backtracking algorithm.
Other aspects that shall be studied are which rules shall be applied in which order and even if some rules are to time consuming to check compared to the extra puzzles that can be solved.
The variations of this algorithm therefore consists of different sets of rules (including guessing rules for backtracking) that are applied in different orders. 

\subsection{Boltzmann machine}
The concept of Boltzmann machines is gradually introduced by beginning with the neuron, network and finally concluding with a discussion on simulation techniques.\\

The central part of an artificial neural network (ANN) is the neuron, as pictured in (TODO).
It decides if to fire an output signal by summing upp all inputs, tresholding the value and applying an activation function producing a binary output value.
In the case of Boltzmann machines the activation function is stochastic and the probablility of a neuron being active is defined as follows:
\[
p_{i=on} = \frac{1}{1+e^{-\frac{\Delta E_{i}}{T}}}
\]
$E_i$ is the summed up energy of the whole network into neuron $i$, which is a fully connected to all other neurons.
T is a temperature constant controlling the rate of change during several evaluations with the probability $p_{i=on}$ during simulation.
$E_i$ is defined as follows \cite{boltzmann2}:
\[
\Delta E_{i} = \sum_{j} w_{ij} s_{j} - \theta
\]
where $s_j$ is a binary value set if neuron $j$ is in a active state, which occurs with probability $p_{i=on}$, and $w_{ij}$ are weights between the current node and node $j$. $\theta$ is a constant offset used to control the overall activation.

The state of every node and the associated weights describes the entire network and encondes the problem to be solved.
In the case of Sudoku there is a need to represent all 81 grid values, each having 9 possible values.
The resulting $81*9=729$ nodes are fully connected and have a binary state which is updated at every discrete timestep.
Some of these nodes will have predetermined outputs since the initial puzzle will fix certain grid values and simplify the problem.
In order to produce valid solutions it is necessary to insert weights describing known relations.
This is done by inserting negative weights, making the interconnected nodes less likely to fire at the same time, resulting in reduced probability of conflicts.
Negative weights are placed in rows, columns, 9x9 subgrids, and between nodes in the same square, since a single square should only contain a single active digit.\\
%Eventually an image here

In order to produce a solution the network is simulated in discrete timesteps.
For every step, all probabilites are evaluated and states are assigned active with the given probability.
Finally the grid is checked for conflicts and no conflicts implies a valid solution, which is gathered by inspecting which nodes are in a active state.

Even though the procedure detailed above eventually will find a solution, there are enhanced techniques used in order to converge faster to a valid solution.
The temperature, $T$, can be controlled over time and is used to adjust the rate of change in the network while still allowing larger mutations to occur.
A typical scheme being used is simulated annealing (TODO: ref).
By starting off with a high temperature (typically $T = 100$) and gradually decreasing the value as time progresses, it is possible to guarantee convergence if the time steps are small enough (TODO: ref).
Due to practical constraints it is not possible to guarantee a solution but simulated annealing provides a good foundation which was used.\\

There are some implications of using a simple linear descent, beginning at a high temperature.
Typically solutions are much less likely to appear in a Boltzmann machine before the temperature has been lowered enough to a critical level.
This is due to the scaling of probabilites in the activation function (TODO: ref to figur).
With a big temperature all probabilites are more or less equal, even though the energy is vastly different.
With a low temperature the exponential function will produce a wider range of values, resuling in increasing probability of ending up with less conflicts.

This will affect the study since the linear temperature decrement was chosen to produce a final value at the end of the simulation.
With increased probability of solutions appearing in the final part of the simulation, this leads to skew towards the end of the spectrum.
Possible workarounds for problems like this are non-linear temperature descents, restarting temperature descent and multiple descents with increasing temperature difference.


\chapter{Method}
%TODO, pabergg
Since this report have several aims, this section have been divided into different parts to clearly depict what aspects have been considered regarding the different aims. 
Those sections will also describe in detail how the results was generated. 
Section 3.1 is devoted to explaining the test setup which includes hardware specifications but also an overview picture of the setup. 
Section 3.2 focuses on how and what aspects of the algorithms where analysed.
Section 3.3 explains the process of choosing test data.
The last subsection 3.4 gives an overview of the statistical analyses which was performed on the test data.
This also includes what computational limitations was present and how this effect the results. 

\section{Test setup}
%TODO HARDWARE SPECIFICATION
The central part of the test setup was the test framework which timed and tested each algorithm on different puzzles. 
This was done by implementing each algorithm as separate classes that all inherited from a superclass for all sudokusolvers.
This made sure the implementations was consistent which made the test process easier.
The last part of the setup consisted of the test puzzles which was tested on the algorithms.
The test consisted of timing the algorithms on each puzzle. 
Since there might be variations in processor performance multiple tests where performed with each puzzle. 
The mean value was then saved for each puzzle and algorithm.
As the variance in processor performance might vary depending on the puzzle tested a confidence intervall for the mean value was calculated for the mean value using bootstraping. 
This will be described in more detail in section 3.4.

\section{Comparison Methods}
Multiple aspects of the results was considered when analysing and comparing the algorithms. The following three subsections describes those aspects in more detail. 
\subsection{Solving}
\subsection{Difficulty rating}
One can often find difficulty ratings associated to Sudoku puzzles in puzzle books etc. Those are often based on the level of human solving techniques that are needed to solve the puzzle in question. \cite{difficulty} 
This study will similarly measure the puzzles difficulty, but will not rely on which level of human solving techniques that are needed, but instead on how well each algorithm performs at solving each puzzle. 
This comparison will be limited to the rule-based and backtracking algorithms and the goal will be to see if there is a correlation between how hard the puzzles are. 
Since only two algorithms will be used, the results will be somewhat uncertain but it will be a beginning in determining if certain puzzles are inherently difficult independent on which algorithms are used to solve it.

\subsection{Generation and parallelization}
This is a more practical aspect of the comparison and no tests will be done. It is however still possible to discuss how well the algorithms are suited for generating puzzle and how well they can be parallelisised. Generation of puzzles is obviously interesting because that is mainly what one want to do if constructing a puzzle collection. Parallelising is however not entirely obvious why it is of interest. Normal Sudoku puzzles can be solved in a matter of milliseconds by the best Sudoku solvers and one might therefore struggle to see the need for parallelising those solvers. And truly this topic is quite unrelevant for normal Sudoku puzzles but the discussion that will be held about the algorithms is however still of practical interest since one might want to solve n by n puzzles which can get extremely difficult fast when n grows. Since the algorithms to some extent also can be applied to other NP-complete problems, the discussion is also relevant in determining which type of algorithms might be useful in other areas. This last remark has already been studied, but not for Sudoku solving algorithms and we therefore take the chance to start the discussion.

\section{Benchmark puzzles}
The test data consisted of multiple puzzles that was chosen beforehand.
Since the set of test puzzles can affect the outcome of this study it is appropriate to motivate the choice of puzzles.
As was discovered during the study the Boltzmann machine algorithm did not perform as well as the other algorithms and some modifications to which puzzles was used was therefore done.
The backtracking and rule-based algorithms was however both tested on a set of 49151 17-clue puzzles. 
Those was found on \cite{database} and is claimed by the author Royle to be a collection of all 17-clue puzzles that he has been able to find on the Internet. 
The reason for chosing this specific database is because the generation of the puzzles does not involve a specific algorithm but is rather a collection of puzzles found by different puzzle generating algorithms.  
The puzzles are therefore assumed to be represantative of all 17-clue puzzles. 
This assumption is the main motivating factor for chosing this set of puzzles, but there is however also other factors that makes this set of puzzles suitable. 
As recently discovered by Tugemann and Civario, no 16-clue puzzle exists which means that puzzles must contain 17 clues to have unique solutions. \cite{17clueProof}  
As discussed under section 3.2.2 difficutly rating is poorly measured by the number of clues in a puzzle, but one can however see a correlation between the number of clues and the difficulty. \cite{difficulty}
This means that the choosen set of 17-clue puzzles shall contain some of the hardest Sudoku puzzles that exists.
This is ofcourse a wanted feature since one then can see how the algorithms performs at puzzles of all difficulties.

\section{Statistical analysis}
Due to several reasons statistical analyses is required to make a rigouros statement about the results. 
This is mainly due to two reason.
Firstly the results contain a very large dataset and secondly there are some randomness in the test results which can only be dealt with by using statistical models. 
Most statistical tests give a confidence in the results to depict how surely one can be about the results of the statistical test. Naturally a higher confidence and more precise results leads to higher requirements on the statistical test. As described in section 3.4.2 some of the statistical tests have been limited by computational constraints and a lower confidence level in combination with a more inprecise result have therefore been needed for those tests.
\subsection{Statistical tests}
This section explains which statistical tests and methods are used in the study.
The first statistical method that is applied is to make sure that variance in processor performance does not affect the results considerable. 
This is done by measuring a specific algorithms solving time for a specific puzzle multiple times. 
The mean value of those times are then calculated and bootstraping are used to attain a 95\% confidence interval of 0.05 seconds. 
The reason bootstraping is used is because it does not require the stochastic variable to be a certain distribution. 
This is necessary since the distribution of the processor perfomance is unkown and also since the distribution might vary between different puzzles. \\
The meanvalues are then saved as described in section 3.1.
It is now that the real analyses of the algorithms begins. 
Even if the representation of the results does not really classify as a statistical method it is appropriate to mention that the results are displayed as histograms which means that the data are sorted and devided into bars of equal width. 
For this study this means each bar represents a fixed size solution time intervall. 
The height of the bars are proportional to the frequency data points falls into that bar's time intervall.
After the histogram are displayed one can easily compare the results between different algorithms and also consider the distribution of the solution times of individual algorithms. The usefullness of this somewhat unrigouros approach shall not negleted, but shall also use statistical methods to rigourosly prove some relations.\\
The first thing one think of looking for is probably how the different algorithms compare in solving ability. 
This means that one want to find out if one algorithm is better than other algorithms. 
Since the distribution again is unknown one has to rely on more general statistical tests. 
One of those are wilcoxons sign test. n
This makes use of the fact that the difference in solving times between two algorithms will have a mean value of 0 if there is no difference between the two algorithms. 
The tests uses the binomial theorem to see if the sign of the difference is unevenly distributed.
One can then determine if an algorithm performs better than an other algorithm with a certain confidence.
Since the distribution of the solution times are unknown it is not possible for wilcoxons sign test to give any measurement of how much better an algorithm is than another algorithm.
One might however make a histogram of the difference which leads us to the next aspect, namely difficulty rating and distribution. 
%Another approach that are also performed are to test if the test data seems to follow a certain distribution. This is done with 
Difficulty rating for each algorithm can easily be calculated by making histograms of the solution times for each algorithm. 
One aim of this study have however been to determine if there is puzzles that are inherently difficult, that is regardless of solution method.
To determine if that is the case some statistical methods are required. The reader shall note that this test is only conducted on the backtracking and rulebased algorithms and the result can therefore not speak for other solution methods.
The method used is however built on the fact that independent events, say A and B, must follow the following property:
\[
P(A \cap B) = P(A) P(B)
\] 
If A is chosen to be the event that a puzzle is within algorithms 1 worst 10\% puzzles.
B is similiarly chosen to be the event that a puzzle is within the 10 \% worst puzzles for algorithm 2.
Since $P(A \cap B)$ shall den be 1 \% a statistical test can be set up to determine if the 10 \% worst puzzles are independent of algorithm. This is done by measuring the probability that the measured results appear if the null hypothesis is ascertained. If the probability is very low one can determine that the worst puzzles are not independent on the algorithms to a certain confidence level.
Similarly the easiest puzzles will be checked for independence.\\
\subsection{Computational constraints}


\section{Benchmark puzzles}
This thesis relies highly on measuring how long it takes different algorithms to solve puzzles. 
To measure this, puzzles are needed and consideration shall therefore be taken to choose those puzzles wisely.
The puzzles that have been chosen to test the solving capabilities of the algorithms are a collection consisting of 49 151 puzzles which each have 17 clues (meaning 17 squares are filled in). \cite{database} Those have been collected by Gordon Royle from different sites as well as from his own database.
The reason for choosing this collection is because it is believed not to be biased towards a specific solving idea.
This is because it is a collection that comes from collecting as many of the 17-clue puzzles that have been found as possible rather than using a specific generation method.
The collection also have other benefits.
The puzzles are all qualitativly different as no pair of puzzles is within the same equivalence class.
Equivalent puzzles are defined as puzzles that could be transformed into each other using some predefined operations that doesnt change the solution.
One might for instance interchange the digits in a puzzle, but the solution would then be the transformed solution and no real change to the puzzle have therefore been done.
Some other operations includes interchanging rows, columns and stacks (three boxes in a row,column).
The downside of choosing this collection is that some algorithms may perform poorly because of the low nr of clues given and some analysis of this will therefore be carried out.
This will be done by solving the given puzzles and adding some of the numbers in the solution to the puzzle.
The reason for this approach is ones again that one does not want to rely too much on a specific generation method.

\section{Comparison methods}
As already mentioned several comparisons and measurements will be carried out to 
analyse different aspects of the algorithms.
Those will include statistically tests 
to for instance determine with what confidence one can say that one algorithm performs 
better than another. 
The following subsections will however explain more clearly what will be measured 
regarding each aspect. 
Two of the general problems one most confront when doing this kind of study is how one 
shall make sure that the measured time for  running a algorithm is correct. 
There might for instance be delays due to slow input and output and one must also account 
to variations in runningtime. 
The input/output delays can be avoided by communicating with the algorithm through a 
direct interface. 
Variations in runningtime is however difficult to predict, but one might however use a 
meanvalue as a approximation. 
To some extent the variations of the runningtimes will be reduced as the algorithms are tested against a large set of puzzles.  

\subsection{Solving}
The main interest of this thesis is the algorithms ability to solve sudokus and most focus will therefore also be on measuring their solving abilities.
As already mentioned, there is a fixed collection of puzzles that shall be tested on each algorithm and also on each variation of the algorithms.
The data needed for the analysis will consist of measuring the time it takes for each algorithm to solve each puzzle.
This will result in quite a big data set and consideration must therefore be taken regarding what statisticall methods are used to analyse it. 
\underline{(It is however at this point not decided which stasticall tests shall be performed so a discussion of that is yet to come).}

\subsection{Generation}
Sudoku solving algorithms may also be used to generate puzzles.
One way this could be done in is by randomly placing numbers in squars and then using the solver to determine if there is a Sudoku and if it is unique.
Some solvers may however be better suited for this as some will for example not be vary suited for determining if a solution is unique.
There are however also other methods for generating puzzles as explained in \cite{generation}.
The algorithms will therefore be compared at theirs ability to generate puzzles in any way that the authors thinks suitable.
The comparison will regard how correct the generated puzzles are (if there is always a unique solution) and also how fast the generation was done.
As the data from the generation testing will be very similar to the data from the solving tests, the same statistical tests will be used.

\subsection{Parallelising}

\subsection{Difficulty rating}

\section{Comparison}

At this point, the conclusions that can be drawn are very limited.
This is largerly due to the fact that the test system have not yet been implemented even thought the first draft of the algorithms have been implemented.
Some tests so far however show that the rule-based solver seems to be able to solve roughly 50\% of the puzzles with only hidden and naked single rules.
Since puzzles that only requires these rules are generally regarded as very easy puzzles, one can conclude that most Sudoku puzzles probably are easy.
An hypothesis that is related to that but not yet tested is that it will probably be difficult to generate difficult problems.
Backtracking algorithms with some variations have also been tested, but at a smaller scale.
It seems so far that backtracking seems to be a less fit algorithm for solving 17-clue puzzles, but some optimization are yet to be done. 
Those are the subsections expected to exist in the finished report.
\subsection{Solving}

This will probably be the most extensive subsection and it will probably be divided into smaller parts.
It is however difficult to tell which those will be now since they greatly depend on what results we get.
\section{Generation}

\section{Parallelising}

\section{Difficulty ratings}

\chapter{Analysis}
%TODO, davnils/pabergg
In this section multiple results will be presented together with a discussion about how the results could be interpreted. 
Section 4.1 is devoted to presenting how the different algorithms performed. 
Section 4.2 explains how the different puzzles can be  

\section{Time distributions}
The time distributions of individual algorithms over the test set shows several things.
One can see if there seem to be no particular difference between puzzles or if some puzzles seem to be more difficult for the algorithm to solve.

\subsection{Rule-based solver}
The rule-based solver was by far the fastest algorithm in the study and also had the lowest standard deviation. 
The result was a mean value of 0.02 seconds and a standard deviation of 0.02 seconds as well. 
The time distribution for the rule-based solver can be seen in 
figure ~\ref{fig:rule-basedDistribution}. 
\begin{figure}[here] 
\noindent\makebox[\textwidth]{%
\begin{tabularx}{1.5\textwidth}{XX}
  \includegraphics[width=20cm]{images/rulebaseddistribution.png}
\end{tabularx}}
\caption{A zoomed in view of the histogram showing the rule-based solvers time distribution among all 49151 puzzles. All puzzles was solved and none had an unstable measurment in running time.}
\label{fig:rule-basedDistribution}
\end{figure}

The figure is a zommed in view of the histogram showing the rule-based solvers time distribution among all 49151 puzzles. All puzzles was solved and all measurements was stable. The maximum solution time was 1.36 seconds but as only very few measurements exceeded 0.2 seconds those have been removed from the figure.
 

\chapter{Conclusions}
%TODO, davnils


\chapter{References}

\begin{thebibliography}{99}

\bibitem{complexity}
Takayuki Y, Takahiro S. Complexity and Completeness of Finding Another Solution and Its Application to Puzzles. [homepage on the Internet]. No date [cited 2012 Mar 8]. Available from: The University of Tokyo, Web site: http://www-imai.is.s.u-tokyo.ac.jp/~yato/data2/SIGAL87-2.pdf

\bibitem{17clueProof}
Tugemann B, Civario G. There is no 16-Clue Sudoku: Solving the Sudoku Minimum Number of Clues Problem. [homepage on the Internet]. 2012 [cited 2012 Mar 8]. Available from: University College Dublin, Web site: http://www.math.ie/McGuire\_V1.pdf

\bibitem{enumeration}
Felgenhauer B, Jarvis F. Enumerating possible Sudoku grids. [homepage on the Internet]. 2005 [cited 2012 Mar 8]. Available from: University of Sheffield, Web site: http://www.afjarvis.staff.shef.ac.uk/sudoku/sudoku.pdf

\bibitem{techniques}
Astraware Limited. Techniques For Solving Sudoku. [homepage on the Internet]. 2008 [cited 2012 Mar 8]. Available from:, Web site: http://www.sudokuoftheday.com/pages/techniques-overview.php

\bibitem{boltzmann}
Ekeberg. Boltzmann Machines. [homepage on the Internet]. 2012 [cited 2012 Mar 8]. Available from:, Web site: http://www.csc.kth.se/utbildning/kth/kurser/DD2432/ann12/forelasningsanteckningar/07-boltzmann.pdf

\bibitem{stochastic}
Marwala T. Stochastic Optimization Approaches for Solving Sudoku. [homepage on the Internet]. 2008 [cited 2012 Mar 8]. Available from:, Web site: http://arxiv.org/abs/0805.0697

\bibitem{review}
. An Incomplete Review of Sudoku Solver Implementations. [homepage on the Internet]. 2011 [cited 2012 Mar 8]. Available from:, Web site: http://attractivechaos.wordpress.com/2011/06/19/an-incomplete-review-of-sudoku-solver-implementations/

\bibitem{discrepancySearch}
Harvey W, Ginsberg M. Limited discrepancy search. [homepage on the Internet]. No date [cited 2012 Mar 13]. Available from: University of Oregon, Web site: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.34.2426\&rep=rep1\&type=pdf

\bibitem{searchBased}
Cazenave Cazenave T. A search based Sudoku solver. [homepage on the Internet]. No date [cited 2012 Mar 13]. Available from: Université Paris, Dept. Informatique Web site: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.64.459\&rep=rep1\&type=pdf

\bibitem{culturalSwarms}
Mantere T, Koljonen J. Sudoku Solving with Cultural Swarms. AI and Machine Consciousness [homepage on the Internet]. 2008 [cited 2012 Mar 13]. Available from: University of Vaasa, Department of Electrical Engineering and Automation Web site: http://www.stes.fi/step2008/proceedings/step2008proceedings.pdf\#page=60

\bibitem{generation}
Morrow J. Generating Sudoku Puzzles as an Inverse Problem. [homepage on the Internet]. 2008 [cited 2012 Mar 13]. Available from: University of Washington, Department of Mathematics Web site: http://www.math.washington.edu/~morrow/mcm/team2306.pdf

\bibitem{database}
Royle G. Minimum Sudoku. [homepage on the Internet]. No date [cited 2012 Mar 13]. Available from: The University of Western Australia, Web site: http://www.math.washington.edu/~morrow/mcm/team2306.pdf

\bibitem{boltzmann2}
Ackley D, Hinton G. A Learning Algorithm for Boltzmann Machines. [homepage on the Internet]. 1985 [cited 2012 Mar 13]. Available from: The University of Western Australia, Web site: http://learning.cs.toronto.edu/~hinton/absps/cogscibm.pdf

\bibitem{difficulty}
%TODO Patrik: fix this reference
%http://www.sudokuwiki.org/Weekly_Sudoku.asp
blargh
\end{thebibliography}

\appendix
\addappheadtotoc
\chapter{RDF}\label{appA}

\begin{figure}[ht]
\begin{center}
And here is a figure
\caption{\small{Several statements describing the same resource.}}\label{RDF_4}
\end{center}
\end{figure}

that we refer to here: \ref{RDF_4}
\end{document}
